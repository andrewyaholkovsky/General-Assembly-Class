{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## Decision Trees (aka Classification and Regression Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read in the data.\n",
    "path = './data/titanic.csv'\n",
    "titanic = pd.read_csv(path)\n",
    "\n",
    "# Encode female as 0 and male as 1.\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
    "\n",
    "# Fill in the missing values for age with the median age.\n",
    "titanic.Age.fillna(titanic.Age.median(), inplace=True)\n",
    "\n",
    "# Create a DataFrame of dummy variables for Embarked.\n",
    "titanic = pd.get_dummies(titanic, columns=['Embarked'])\n",
    "\n",
    "\n",
    "# create predictor matrix and target\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S','Embarked_C']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# instantiate - random_state for reproducibility, max_depth as a stopping criterion\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "# fit\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does a Computer Build a Tree?\n",
    "\n",
    "1. Begin at the top of the tree.\n",
    "2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint so that the resulting tree has the greatest decrease in impurity. Make that split.\n",
    "3. Examine the two resulting regions. Once again, make a **single split** (in one of the regions) to minimize the MSE.\n",
    "4. Keep repeating Step 3 until a **stopping criterion** is met:\n",
    "    - Maximum tree depth (maximum number of splits required to arrive at a leaf).\n",
    "    - Minimum number of observations in a leaf.\n",
    "    - All the points are classified\n",
    "\n",
    "---\n",
    "**Ideal approach:** Considering every possible partition of the feature space (computationally infeasible).\n",
    "\n",
    "**\"Good enough\" approach:** Take the best split at each step (recursive binary splitting.)\n",
    "\n",
    "This is a **greedy algorithm** because it makes locally optimal decisions -- it takes the best split at each step. A greedy algorithm hopes that a series of locally optimal decisions might be optimal overall; however, this is not always the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tree for Titanic data](assets/tree_titanic.png)\n",
    "\n",
    "- The **maximum value** of the Gini index is 0.5 and occurs when the classes are perfectly balanced in a node.\n",
    "- The **minimum value** of the Gini index is 0 and occurs when there is only one class represented in a node.\n",
    "- A node with a lower Gini index is said to be more \"pure.\n",
    "\n",
    "- how would a decision tree **regressor** assess how well separated the data is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model\n",
    "####  The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. In other words, the higher the feature importance, the more effective the feature was able to split the data to create more pure splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_fi = pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})\n",
    "dtc_fi.sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "# decision trees have lots of hyperparameters that we could potentially optimize\n",
    "# using cross validation to optimize hyperparameters (tune our model)\n",
    "for depth in [2,6,None]:\n",
    "    treeclf = DecisionTreeClassifier(max_depth=depth)\n",
    "    scores = cross_val_score(treeclf, X, y, cv=5)\n",
    "    print('at max_depth of '+str(depth)+': '+str(np.mean(scores))+ ' +/- ' +str(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we don't establish a stopping criterion, what is a decision tree going to do? \n",
    "\n",
    "### Add decision trees to machine learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "https://www.youtube.com/watch?v=loNcrMjYh64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ensembling?\n",
    "\n",
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model. For example, given predictions from several models we could:\n",
    "\n",
    "- **Regression:** Take the average of the predictions.\n",
    "- **Classification:** Take a vote and use the most common prediction.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "A **bootstrap sample** is a random sample with replacement. Think flipping a coin as opposed to hitting shuffle on a playlist. So, it has the same size as the original sample but might duplicate some of the original observations.\n",
    "\n",
    "**Bagging** is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create an array of 1 through 20.\n",
    "nums = np.arange(1, 21)\n",
    "print(nums)\n",
    "\n",
    "# Sample that array 20 times with replacement.\n",
    "print(np.random.choice(a=nums, size=20, replace=True))\n",
    "\n",
    "# For each tree, the **unused observations** are called \"out-of-bag\" observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does bagging work (for decision trees)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest\n",
    "\n",
    "1. Grow B trees using B bootstrap samples from the training data.\n",
    "2. Train each tree on its bootstrap sample \n",
    "3. When building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**. A new random sample of features is chosen for **every single tree at every single split**.\n",
    "3. Combine the predictions:\n",
    "    - Average the predictions for **regression trees**.\n",
    "    - Take a vote for **classification trees**.\n",
    "\n",
    "\n",
    "**What's the point?**\n",
    "\n",
    "* If every batch of test data was exactly like our training data, what would be better, CART or Random Forest?\n",
    "* Why bother with Random Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best = RandomForestClassifier(max_depth=4, max_features=4, min_samples_split=5)\n",
    "rfc_best.fit(X, y)\n",
    "rfc_best.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature':feature_cols, 'importance':rfc_best.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add decision trees to machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
