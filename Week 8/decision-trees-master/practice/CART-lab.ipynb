{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Visualizing CARTs with admissions data\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "Using the admissions data from earlier in the course, build CARTs, look at how they work visually, and compare their performance to more standard, parametric models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Load in admissions data and other python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#from ipywidgets import *\n",
    "#from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admit = pd.read_csv('../data/admissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Create regression and classification X, y data\n",
    "\n",
    "The regression data will be:\n",
    "\n",
    "    Xr = [admit, gre, prestige]\n",
    "    yr = gpa\n",
    "    \n",
    "The classification data will be:\n",
    "\n",
    "    Xc = [gre, gpa, prestige]\n",
    "    yc = admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Cross-validate regression and logistic regression on the data\n",
    "\n",
    "Fit a linear regression for the regression problem and a logistic for the classification problem. Cross-validate the rmse and accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Building regression trees\n",
    "\n",
    "With `DecisionTreeRegressor`:\n",
    "\n",
    "1. Build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the R2 scores of each of the models and compare to the linear regression earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Building classification trees\n",
    "\n",
    "With `RandomForestClassifier`:\n",
    "\n",
    "1. Again build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the accuracy scores of each of the models and compare to the logistic regression earlier.\n",
    "\n",
    "Note that now you'll be using the classification task where we are predicting `admit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Print out the \"feature importances\"\n",
    "\n",
    "The model has an attribute called `.feature_importances_` which can tell us which features were most important vs. others. It ranges from 0 to 1, with 1 being the most important.\n",
    "\n",
    "An easy way to think about the feature importance is how much that particular variable was used to make decisions. Really though, it also takes into account how much that feature contributed to splitting up the class or reducing the variance.\n",
    "\n",
    "A feature with higher feature importance reduced the criterion (impurity) more than the other features.\n",
    "\n",
    "Below, fit a decision tree classifer with the best max_depth parameter and show the feature importances for each variable predicting private vs. not, sorted by most important feature to least. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you're wondering whether there is a better way to tune hyperparameters than fitting individual models, yes, there is and we'll get there soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
