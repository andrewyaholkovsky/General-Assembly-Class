{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Primer\n",
    "https://youtu.be/yIYKR4sgzI8?t=181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>glass_type</th>\n",
       "      <th>household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.1</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ri     na    mg   al     si     k    ca   ba   fe  glass_type  \\\n",
       "0  1.52101  13.64  4.49  1.1  71.78  0.06  8.75  0.0  0.0           1   \n",
       "\n",
       "   household  \n",
       "0          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass = pd.read_csv('data/glass.csv',names=[\n",
    "    'ri','na','mg','al','si','k','ca','ba','fe','glass_type'],skiprows=1)\n",
    "# Glass Types 1, 2, 3 in are window glass.\n",
    "# Types 5, 6, 7 are household glass.\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate classifier\n",
    "logr = LogisticRegression(solver='lbfgs')\n",
    "# set up predictor matrix\n",
    "X = glass[['al']]\n",
    "# set up target variable\n",
    "y = glass.household\n",
    "# fit classifier\n",
    "logr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The logit function\n",
    "\n",
    "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n",
    "\n",
    "The intercept plus the coefficient times a value for `al` equals our log odds for a given observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer on Log Odds\n",
    "To understand how logistic regression predicts the probability of class membership we need to start by understanding the relationship between probability, odds ratios, and log odds ratios. This is because logistic regression predicts log odds and so reading log odds is necessary for interpreting logistic regression.\n",
    "\n",
    "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "\n",
    "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dice Roll of 1')\n",
    "print('probability: 1/6')\n",
    "print ('odds: 1/5')\n",
    "print ('log odds: '+str(np.log(1/5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Even Dice Roll')\n",
    "print('probability: 3/6')\n",
    "print ('odds: 3/3')\n",
    "print ('log odds: '+str(np.log(3/3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds can never be negative but log odds can\n",
    "print(np.log(5/2))\n",
    "print(np.log(2/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can exponentiate log odds to get back to odds\n",
    "# from odds we can easily get back to probability\n",
    "print(np.exp(np.log(2/5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting logistic regression attributes\n",
    "    * our logistic regression, like linear regression, has coefficients and an intercept\n",
    "    * but in logistic regression, our coefficients and intercept are expressed in log odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('logr intercept: '+str(logr.intercept_[0]))\n",
    "print('al coef: '+str(logr.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when aluminimum equals zero, what are the log odds of household = 1?\n",
    "* for every unit of aluminum gained, what is the log odds increase of household = 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** To really interpret our equation, we need to convert log odds to probabilities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert log odds to probability\n",
    "def log_to_prob(al_value):\n",
    "    # add coef, aluminum value, intercept\n",
    "    logodds = logr.coef_[0][0]*al_value+logr.intercept_[0]\n",
    "    # exponentiate sum  - the inverse of taking the log - to get odds\n",
    "    odds = np.exp(logodds)\n",
    "    # convert odds to probability\n",
    "    prob = odds/(1 + odds)\n",
    "    # return probability\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p value when al is 2.5: '+str(log_to_prob(2.5)))\n",
    "print('p value when al is 0: '+str(log_to_prob(0)))\n",
    "print('change in probability: '+str(log_to_prob(2.5)-log_to_prob(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predict proba method does this work for us\n",
    "logr.predict_proba([[2.5]])[:,1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predicted probabilities of class 1 - below I'm taking all the rows and the second column\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "glass['household_pred_prob'] = logr.predict_proba(X)[:, 1]\n",
    "glass.sort_values('household_pred_prob',inplace=True)\n",
    "# Plot the predicted probabilities.\n",
    "plt.scatter(glass.al, glass.household)\n",
    "plt.plot(glass.al, glass.household_pred_prob, color='red')\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household_proba')\n",
    "plt.show()\n",
    "# what's the probability of household = 1 when aluminum is 1.5? 2.0? 3.5?\n",
    "# what about if we got an aluminiumum value of 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencing the modeling framework, create a row for logistic regression in your own words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Confusion Matrix and Other Classification Metrics\n",
    "\n",
    "$$Accuracy = \\frac{total~predicted~correct}{total~predicted}$$\n",
    "\n",
    "Accuracy alone doesn’t always give us a full picture.\n",
    "\n",
    "If we know a model is 75% accurate, it doesn’t provide any insight into why the 25% was wrong.\n",
    "\n",
    "Consider a binary classification problem where we have 165 observations/rows of people who are either smokers or nonsmokers.\n",
    "\n",
    "- **True positives (TP):** These are cases in which we predicted yes (smokers), and they actually are smokers.\n",
    "- **True negatives (TN):** We predicted no, and they are nonsmokers.\n",
    "- **False positives (FP):** We predicted yes, but they were not actually smokers. (This is also known as a \"Type I error.\")\n",
    "- **False negatives (FN):** We predicted no, but they are smokers. (This is also known as a \"Type II error.\")\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# high accuracy\n",
    "preds = logr.predict(X)\n",
    "accuracy_score(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conmat = confusion_matrix(y, preds)\n",
    "confusion = pd.DataFrame(conmat, index=['Actual: 0','Actual: 1'],\n",
    "                         columns=['Pred: 0','Pred: 1'])\n",
    "# precision (TP/(TP+FP)) is good - we're correct 26 of 29 times we predicted a positive\n",
    "# but middling recall (TP/(TP+FN)) - only 26 of 51 positive cases correct\n",
    "# an f1 score is a harmonic mean of these two metrics\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### there are cases where it's desirable to have a lower threshold than .5, typically where false negatives might be extremely undesirable, and we can reduce them by tolerating more false positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = logr.predict_proba(X)[:,1]\n",
    "thresh = .4\n",
    "# predict that any pred over thresh is a positive\n",
    "preds = pd.Series(probas).map(lambda x: 1 if x>=thresh else 0)\n",
    "# score in conmat\n",
    "conmat = confusion_matrix(y, preds)\n",
    "confusion = pd.DataFrame(conmat, index=['Actual: 0','Actual: 1'],\n",
    "                         columns=['Pred: 0','Pred: 1'])\n",
    "# at a lower threshold, we increase our predicted TPs by 6, as well as our FPs by 6\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
